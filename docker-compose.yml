version: '3.8'

# common services for inference on CPU and GPU
x-inference-common-service: &inference-common-service
  build: 
    context: .
    dockerfile: ./inference/Dockerfile
  shm_size: "8gb"
  volumes:
    - ./inference:/home/feyntune-inference
  ports:
    # Jupyter
    - 8888:8888
    # vllm inference server
    - 6969:6969
  environment:
    - TESTING=0
    - LOCAL_DEV=1
  command: poetry run poe jl

# common services for training on CPU and GPU
x-train-common-service: &train-common-service
  build:
    context: . 
    dockerfile: ./train/Dockerfile
    target: runner
    args:
      HUGGINGFACE_KEY: ${HUGGINGFACE_KEY}
  shm_size: "8gb"
  volumes:
    - ./train:/home/feyntune-train
  environment:
    HUGGINGFACE_KEY: ${HUGGINGFACE_KEY}
  ports:
    # Jupyter
    - 8889:8888
  command: poetry run poe jl

services:
  # -- Inference server
  # GPU
  inference-gpu:
    <<: *inference-common-service
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    image: feyntune-inference-gpu
    container_name: feyntune-inference-gpu-container
  # CPU-only
  inference-cpu: 
    <<: *inference-common-service
    image: feyntune-inference-cpu
    container_name: feyntune-inference-cpu-container
  # -- Finetuning scripts
  # GPU
  train-gpu:
    <<: *train-common-service
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    container_name: feyntune-train-gpu-container
    image: feyntune-train-gpu
  # CPU-only
  train-cpu:
    <<: *train-common-service
    image: feyntune-train-cpu
    container_name: feyntune-train-cpu-container