version: '3.8'

x-common-service: &common-service
  build: 
    context: .
    dockerfile: ./dockerfiles/Dockerfile
  shm_size: "8gb"
  volumes:
    - .:/home/feyntune
  ports:
    # Jupyter
    - 8888:8888
    # model inference server
    - 6969:6969
    # Tensorboard
    - 8686:8686
    - 6006:6006
    # MLFlow
    - 5000:5000
  expose:
    - 5000
  environment:
    - TESTING=0
    - LOCAL_DEV=1
    - MLFLOW_TRACKING_URI=http://0.0.0.0:5000
    - INFERENCE_SERVER_HOST=0.0.0.0
    - MODEL=bigscience/bloomz-560m
  command: /bin/sh -c "python -m vllm.entrypoints.openai.api_server --host $INFERENCE_SERVER_HOST --port 6969 --model $MODEL"

services:
  gpu:
    <<: *common-service
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    container_name: feyntune_gpu_container
  cpu: 
    <<: *common-service
    container_name: feyntune_cpu_container